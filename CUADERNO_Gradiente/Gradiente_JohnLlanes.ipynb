{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=text-center >\n",
    "<h2>Gradiente</h2>\n",
    "</div>\n",
    "\n",
    "### Nombre: John Llanes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema de aprendizaje en las redes neuronales se formula en términos de la minimización de la función de error (o pérdida) asociada, y que notaremos en esta entrada por medio de f.\n",
    "\n",
    "Normalmente, esta función está compuesta por dos términos, uno que evalúa cómo se ajusta la salida de la red neuronal al conjunto de datos de que disponemos, y que se denomina término de error, y otro que se denomina término de regularización, y que se utiliza para evitar el sobreaprendizaje por medio del control de la complejidad efectiva de la red neuronal.\n",
    "\n",
    "Por supuesto, el valor de la función de error depende por completo de los parámetros de la red neuronal: los pesos sinápticos entre neuronas, y los bias asociados a ellas, que, como suele ser ya habitual, se pueden agrupar adecuadamente en un único vector de peso de la dimensión adecuada, que denotaremos por w. En este sentido, podemos escribir f(w) para indicar que el valor del error que comete la red neuronal depende de los pesos asociados a la misma. Con esta formalización, nuestro objetivo es encontrar el valor w∗ para el que se obtiene un mínimo global de la función f, convirtiendo el problema de aprendizaje en un problema de optimización, como va siendo habitual.\n",
    "\n",
    "En general, la función de error es una función no lineal, por lo que no disponemos de algoritmos sencillos y exactos para encontrar sus mínimos. En consecuencia, tendremos que hacer uso de una búsqueda a través del espacio de parámetros que, idealmente, se aproxime de forma iterada a a un (error) mínimo de la red para los parámetros adecuados.\n",
    "\n",
    "![Alt Text](http://www.cs.us.es/~fsancho/images/2017-02/gds.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Descenso del Gradiente es el algoritmo de entrenamiento más simple y también el más extendido y conocido. Solo hace uso del vector gradiente, y por ello se dice que es un método de primer orden.\n",
    "\n",
    "Este método para construir el punto wi+1 a partir de wi se traslada este punto en la dirección de entrenamiento di=−gi. \n",
    "\n",
    "Es decir:\n",
    "\n",
    "$ \\w_{i + 1} = w_i - g_i \\nu_i\n",
    "\n",
    "Donde el parámetro ν\n",
    "se denomina tasa de entrenamiento, que puede fijarse a priori o calcularse mediante un proceso de optimización unidimensional a lo largo de la dirección de entrenamiento para cada uno de los pasos (aunque esta última opción es preferible, a menudo se usa un valor fijo, νi=ν\n",
    "\n",
    "con el fin de simplificar el proceso).\n",
    "\n",
    "Aunque es muy sencillo, este algoritmo tiene el gran inconveniente de que, para funciones de error con estructuras con valles largos y estrechos, requiere muchas iteraciones. Se debe a que, aunque la dirección elegida es en la que la función de error disminuye más rápidamente, esto no significa que necesariamente produzca la convergencia más rápida.\n",
    "\n",
    "![Alt Text](http://www.cs.us.es/~fsancho/images/2017-02/gradient_descent.gif)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sillanes",
   "language": "python",
   "name": "sillanes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
